Transformers with Learnable Activation Functions    
激活函数可以显著降低输入数据的拓扑复杂度，从而提高模型的性能。选择合适的激活函数是神经模型设计中的一个重要步骤。然而，在基于Transformer的语言模型中，激活函数的选择很少被讨论或探讨。它们的激活函数是预先选择的，然后从预训练到微调保持固定。因此，在这个漫长的生命周期中，他们施加在模型上的感应偏差无法调整。此外，随后开发的模型（例如，RoBERTa、BART和GPT-3）经常跟进之前的工作（例如：BERT）使用相同的激活函数，而不进行调整。本文研究了在Transformer体系结构中使用有理激活函数（RAF）的有效性。RAF是一种可学习的激活函数。与传统的预先定义的激活函数相比，RAFs可以在训练过程中根据输入数据自适应地学习最优激活函数。实验结果表明，基于RAF的Transformer（RAFT）算法的验证复杂度低于传统的BERT算法。我们进一步评估了RAFT在低数据和全数据设置下的下游任务。我们的结果表明，RAFT在大多数任务和环境下都优于对应的模型。例如，RAFT在GLUE基准测试中的平均性能比普通BERT高5.71个点（低数据场景中有100个训练样本可用），在SQuAD中的平均性能比普通BERT高2.05个点（全数据设置）。对学习到的RAF的形状的分析进一步揭示了它们在预训练模型的不同层之间实质上不同，并且大多数看起来与传统的激活函数非常不同。RAFT为根据学习到的激活函数分析和解释预训练模型开辟了一个新的研究方向。    
对transformer激活函数的改进

------
