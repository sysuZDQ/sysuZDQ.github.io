Efficient Multimodal Transformer with Dual-Level Feature Restoration for Robust Multimodal Sentiment Analysis     
随着用户生成的在线视频的激增，多模态情感分析（MSA）最近引起了越来越多的关注。尽管取得了重大进展，但在实现稳健MSA的道路上仍存在两大挑战：1）当在未对准的多模态数据中建模跨模态交互时效率低;本文提出了一个通用的、统一的、具有双层特征恢复的高效多模态Transformer（Efficient Multimodal Transformer with Dual-Level Feature Restoration，EMT-DLFR）框架，EMT将每种模态的话语级表征作为全局多模态语境，与局部单模态特征进行交互，相互促进。为了提高模型在不完备模态环境下的鲁棒性，DLFR一方面进行低层特征重构，以隐含地激励模型从不完备数据中学习语义信息，另一方面，它创新性地将完整和不完整数据视为同一样本的两个不同视图，并利用连体表示学习来显式地吸引它们的高级表示。在三个常用数据集上的实验表明，该方法在完整和不完整模态下都具有较好的性能选项卡页中出现    
多模态情感分析   

-----
Mask and Reason: Pre-Training Knowledge Graph Transformers for Complex Logical Queries   
知识图（KG）嵌入是不完备KG推理的主流方法，但由于其固有的浅层和静态结构，难以处理由逻辑运算符、输入边、多个源实体和未知中间实体组成的复杂逻辑查询.本文提出了一种基于嵌入的不完备KG推理方法，我们介绍知识图Transformer（kgTransformer）算法，设计了一种KG三重变换方法，使Transformer能够处理KG，专家混合小组进一步加强了这一点（MoE）稀疏激活算法，将复杂逻辑查询转化为掩蔽预测，并引入两阶段掩蔽预训练策略，提高了算法的可移植性和泛化能力.在两个基准测试上的大量实验表明，kgTransformer在9个领域内和领域外推理任务上的性能始终优于基于KG嵌入的基准和高级编码器，并且kgTransformer通过提供完整的推理路径来解释给定的答案，具有可解释性.     
知识图谱和transformer的结合 

------
