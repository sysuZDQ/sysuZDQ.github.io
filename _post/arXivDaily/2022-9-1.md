A topic-aware graph neural network model for knowledge base updating    
开放领域知识库是非常重要的。它通常是从百科全书网站中提取出来的，广泛应用于知识检索系统、问答系统或推荐系统中。在实践中，关键的挑战是保持最新的知识库。与从百科全书转储中获取所有数据的笨拙方式不同，为了尽可能地增加知识库的新鲜度，同时避免无效获取，现有的知识库更新方法通常通过建立预测模型来判断实体是否需要更新。但由于数据来源和数据结构的问题，这些方法只能在特定领域进行定义，结果往往存在明显的偏差。对于开放领域知识，用户的查询意图往往是多样的，因此我们基于用户的查询日志构建了一个主题感知图网络来进行知识更新。我们的方法可以总结如下：1.通过用户的日志提取实体并选择它们作为种子2.在百科网站中抓取种子实体的属性，并为每个实体自监督构建实体属性图。3.使用实体属性图来训练GNN实体更新模型，以确定实体是否需要同步。4.Use 百科知识库，根据最小编辑次数算法，将过滤后的实体与知识库中的实体进行匹配更新。     
如何高效地更新知识库还是一个非常具有实用价值的问题

-----
MultiCoNER: A Large-scale Multilingual dataset for Complex Named Entity Recognition   
我们介绍MultiCoNER，这是一个用于命名实体识别的大型多语言数据集，涵盖了11种语言的3个领域（Wiki语句、问题和搜索查询），以及多语言和代码混合子集。该数据集旨在代表当代的NER挑战，包括低上下文场景（短文本和无大小写文本）、句法复杂的实体（如电影标题）和长尾实体分布。26M令牌数据集是使用诸如基于启发式的句子采样、模板提取和开槽以及机器翻译等技术从公共资源编译的。我们在数据集上应用了两个NER模型：基线XLM-RoBERTa模型和利用地名录的最先进的GEMNET模型。基线达到中等性能（macro-F1 =54%），突出了我们数据的难度。使用地名索引的全球环境监测网有了显著改进（宏观F1平均改进=+30%）。MultiCoNER即使对于大型的预先训练的语言模型也是一个挑战，我们相信它可以帮助进一步研究建立健壮的NER系统。MultiCoNER可在www.example.com上公开https: registry.opendata.aws multiconer 获取，我们希望此资源将有助于推进NER各个方面的研究。    
大型多语言数据集

----
