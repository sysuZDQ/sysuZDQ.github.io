TSRFormer: Table Structure Recognition with Transformers   
提出了一种新的表格结构识别方法（TSR）方法，称为TSRFormer，用于从各种表格图像中鲁棒地识别具有几何失真的复杂表格结构.与以往方法不同，我们将表格分隔线预测问题表述为线性回归问题而不是图像分割问题，并提出了一种新的基于DETR的两阶段分隔线预测方法，称为 textbf{Separator textbf{RE}gression textbf{TR}ansformer（SepRETR），用于直接从表格图像预测分隔线。为了使两阶段DETR框架高效且有效地工作于分隔线预测任务，我们提出两个改进：1）采用先验增强的匹配策略解决DETR算法收敛速度慢的问题; 2）新的交叉关注模块直接从高分辨率卷积特征图中采样特征，从而以较低的计算代价获得较高的定位精度;在分离线预测之后，使用基于简单关系网络的单元合并模块恢复跨越单元.此外，我们还在更具挑战性的真实内部数据集上验证了我们的方法对具有复杂结构、无边界单元格、大空白空间、空或跨越单元格以及扭曲甚至弯曲形状的表格的鲁棒性。   
处理表格的transformer

------
Design of High-Throughput Mixed-Precision CNN Accelerators on FPGA   
卷积神经网络（CNNs）在各种应用领域都能达到很高的精度，但需要大量的计算量和昂贵的数据移动.一种在保证精度的同时降低这些成本的方法是减少权值和 或激活字长.因此，逐层混合精度量化在增大设计空间的同时允许更有效的结果.在这项工作中，我们提出了一种深入的定量方法来有效地探索设计空间，考虑到给定FPGA的有限硬件资源，我们的整体探索方法垂直地遍历了从架构到逻辑级别的各个设计入口级别，和通道量化的CNN的高效执行，实现了真正的混合精度运算，实现了映射前馈和身份验证，实现了混合精度的CNN加速器。快捷连接混合精度CNN导致了具有竞争力的精度—吞吐量折衷：245帧 秒，ResNet-18的Top-5精度为87.48%，ResNet-152的Top-5精度为92.9%，TOps 秒。因此，与相应的浮点基线相比，参数所需的内存占用量减少了4.9倍和9.4倍。   
加速CNN模型

-----
Investigating Efficiently Extending Transformers for Long Input Summarization   
虽然大型预训练Transformer模型已被证明在处理自然语言任务方面具有很高的能力，但处理长序列输入仍然是一个重大挑战。其中一个任务是长输入摘要，其中输入比大多数预训练模型的最大输入上下文长。通过一组广泛的实验，我们研究了什么模型结构变化和预训练范例可以最有效地使预训练Transformer适应长输入摘要。我们发现，基于我们的发现，我们引入了PEGASUS-X算法，它可以有效地提高数据摘要的性能，PEGASUS模型的扩展，具有额外的长输入预训练，可处理多达16K令牌的输入。X在长输入汇总任务上实现了与大得多的模型相当的强大性能，同时添加了很少的附加参数，并且不需要模型并行性来训练。    
解决transformer对长文本输入的问题 

-----
Emotion Detection From Tweets Using a BERT and SVM Ensemble Model  
自动识别Twitter数据中表达的情绪有着广泛的应用，我们通过在包含四种情绪的基准数据集上添加一个中性类来创建一个平衡的数据集：恐惧、悲伤、喜悦和愤怒。在这个扩展的数据集上，我们研究了支持向量机（SVM）和来自Transformers的双向编码器表示（BERT）在情感识别中的应用。我们将BERT和SVM模型结合起来，提出了一种新的集成模型。实验表明，所提出的模型在推特的情感识别中达到了0.91的最高准确率。  
BERT和传统机器学习模型（SVM）的结合

---
 On the Activation Function Dependence of the Spectral Bias of Neural Networks   
 神经网络是一种普适函数逼近器，尽管它具有显著的过参数化，但却具有很好的泛化能力.本文从神经网络的谱偏差的角度研究了这一现象.我们的贡献有两个：第一，利用与有限元理论的联系，对ReLU神经网络的谱偏差提供了理论解释;第二，基于该理论，我们预测将激活函数转换为分段线性B样条，即帽函数，将消除该谱偏差，我们的经验研究也表明，使用随机梯度下降和ADAM，具有Hat激活函数的神经网络的训练速度明显更快。结合之前的工作表明，帽子激活函数也提高了图像分类任务的泛化精度，这表明在某些问题上，使用帽子激活函数比ReLU具有显著的优势。   
 他说他这个帽子激活函数比ReLU好

 -----
 