## 摘要
提出了一个模型Omni-perception Pre-Trainer(OPT)，通过利用文本、图像、语音数据实现了跨模态的理解和生成！      
OPT使用的是encoder-decoder框架，包括了    
- 三个singel-modal encoders：分别编码三种模态
- 一个cross-modal encoder：编码三种模态的联系
- 两个cross-modal decoders：通过自回归的方式分别生成文本和图像

<div align=center><img src="..\深度调研\8bf0c376f9f4541be06c019131815e2b_2_Figure_1.png" width="500"></div>

在预训练任务方面，使用了三种数据粒度的任务
- token-level:predicts the semantics of masked tokens given the unmasked inputs
- modality-level: two generative tasks, i.e., denoising text reconstruction and denoising image reconstruction and  randomly masks out the whole inputs from any one or two of the three modalities
- sample-level: learns the alignment among the three modalities corresponding to the same sample
  
数据集上，使用了Open Images的image-text-audio triplets     

## 介绍   
这项工作的动机是 *a machine with human-like intelligence should be trained on multi-modal resources, to develop the both capabilities of cross-modal understanding and generation.*    
先前的多模态预训练工作存在一个问题： *they are proposed to specialize in certain types of cross-modal understanding or generation tasks, and cannot establish general knowledge for unified processing.*例如ViLBERT、VisualBERT只做理解任务，而DALL-E只能做生成任务。    
为了验证实验结果，做了大量的下游任务，包括 *cross-modal retrieval, multi-modal classification, visual question answering, cross-modal text generation (including speech recognition and visual captioning), and text-to-image generation*    
总的来说，OPT的创新点有
-  OPT is the first pre-trained model that connects the three modalities of text, vision, and audio, and is endowed with the both capacities of cross-modal understanding and generation.
-   OPT learns to align and translate among different modalities with the token-, modality-, and samplelevel pretext tasks.
-    OPT can effectively adapt to and perform competitively on a series of cross-modal understanding and generation downstream tasks with parial or all modalities as inputs.   

## 相关工作   
### Single-Modal Pre-Training
- NLP：成功的关键在于Transformer架构的使用和一些在大规模语料库上的预训练任务，一些优秀工作有GPT,BERT,XLNet,MASS,UniLM,BART，一个想法是参考这些工作的思路，运用到我们这个三模态预训练模型上。一个需要考虑的问题是，如果直接迁移肯定有gap，那么我们需要解决这个问题
- CV：最近来说，对比学习是一个值得深入的方向，相关工作有SimCLR,MoCo,BYOL
- Audio and Speech；这块我不是很了解 pre-training has focused on emotion recognition [24], speaker identification [32], phoneme discrimination [39, 27], transferring ASR representations from one language to another [17], unsupervised representations learning for speech [35], audio representation learning [43].   

### Mutil-Modal Pre-Training
以前的多模态预训练都是VLP，主要可以分为两种类型
- 单流： VisualBERT [22], UNITER [5], Unicoder-VL [20] and VL-BERT [38]
- 双流： ViLBERT [25] and LXMERT [40]

因此，OPT是第一个三模态预训练模型

## 模型架构
### Single-Modal Encoders
- Text Encoder:BERT
- Vision Encoder:Faster R-CNN
- Audio Encoder:wav2vec 2.0

### Cross-Modal Encoder
将三个模态的embedding拼接起来，然后通过一个编码器从而得到表示$M$    
$$M=CrossEncoder([T;V;A])$$

### Cross-Modal Decoders
这两个解码器都是将重构输入的文本和图像作为预训练任务，将生成结果作为下游任务
- Text Decoder：使用了Transformer Decoder
- Vision Decoder：使用了two-stage框架

<div align=center><img src="..\深度调研\8bf0c376f9f4541be06c019131815e2b_4_Figure_2.png" width="500"></div>

## 预训练任务
### Token-Level Modeling
- MLM:mask 15%的文本token，利用其它数据来预测这些masked tokens
  $$L_MLM(\theta)=-E_{(T,V,A)~D}logP_\theta(T_m|T_{\\ m},V,A)$$
- MVM:和MLM不一样的是，视觉特征是高维和连续的，音系不适合使用类极大似然作为目标函数，这里使用了两个目标函数MVFR和MRC
  $$L_{MVM}(\theta)=E_{(T,V,A)~D}f_\theta(V_m|T,V_{\\ m},A)$$  
   $$ MVFR:f_\theta(V_m|T,V_{\\ m},A)=\sum_m||h_\theta(V_m)-V_m||_2^2 $$
  $$MRC:f_\theta(V_m|T,V_{\\ m},A)=\sum_mCE(g_\theta(V_m),gt(V_m))$$
- MAM:使用了L2回归损失函数和一个对比学习
  $$L_{MVM}(\theta)=E_{(T,V,A)~D}f_\theta(A_m|T,V,A_{\\ m})$$
  $$MAFR:f_\theta(A_m|T,V,A_{\\ m})=\sum_m||h_\theta(A_m)-A_m||_2^2$$
  $$CL:f_\theta(A_m|T,V,A_{\\ m})=-log \frac{exp(sim(h_\theta(A-m),A_m))}{exp(sim(h_\theta(A_m),A_m)+exp(sim(h_\theta(A_m),A_{\\ m})))}$$

### Modality-Level Modeling
这部分包括文本和图片的重构，通过设计在一个样本中mask掉一个或者两个模态的任务，有利于让OPT应对一个、两个三个模态输入的情况
- DTR
  $$L_{DTR}(\theta)=-E_{(T,V,A)~D}logP_{\theta}(\hat T_i|\hat T_{< i},V,A)$$
- DIR
   $$L_{DIR}(\theta)=-E_{(T,V,A)~D}logP_{\theta}(\hat I_i|\hat I_{< i},T,V,A)$$

### Sample-Level Modeling
具体做法是随机替换一个样本中的一个或者两个模态，让模型学会判断样本中模态的匹配情况如何（一共有五种情况）用一个[CLS]来作为提取的特征（五分类问题？）   
 We denote the output scores as $s_θ (T, I, A) ∈ R^5$, $gt(T, V, A)$ is the one-hot vector of ground-truth label.
$$L_{SM}(\theta)=E_{(T,V,A)~D}BCE(s_\theta(T,V,A),gt(T,V,A))$$
## 实验
### Pre-Training Dataset
### Implementation Details
### Results
### Ablation Study
### Qualitative Results
 It is noted that we make a first attempt to incorporate the image generation into the pre-trained model. There is still room to improve the image decoder and image reconstruction pre-training task, which we leave for future work.
## 讨论
论文提出了五点值得深究的地方
- 收集三模态数据并不容易
- 如何在 un-paired or partial-modality data的情况下，也就是某些模态缺失的情况下有效地训练模型（个人觉得这里可以参考先前一些VLP中的解决方案
- 在预训练过程中如何利用好知识（这里是不是可以加入诸如知识图谱这一类的外部知识
- 通过人类的反馈和交互来进一步学习，也就是强化学习
- 开发更多的应用，这里指的是论文中没有做的一些应用，如 generate raw audio (conditioned on language, image, or another audio), image editing and image to image translation, video editing and generation under language/audio instructions etc.
