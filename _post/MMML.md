# Multimodal Machine Learning
我们所处的世界是多模态的，我们可以看见、听见，摸到、闻到、尝到......   
显然，如果我们的模型也能像人类一样，从多个角度去观察事物，那么它得到的信息肯定更加地全面、准确。   
目前来说，MMML面临的挑战包括 representation, translation, alignment, fusion, and co-learning   
当下，我们主要关注三种模态：既可以写也可以说的自然语言；通常用图像或视频表示的视觉信号；编码声音和副词信息的声音信号，如韵律、声乐等。
具体来说，这五个挑战是
- **表示**：第一个基本挑战是学习如何以一种利用多种模态的互补性和冗余性的方式表示和汇总多模式数据。多模数据的异构性使得构造这样的表示方法具有挑战性。例如，语言通常是象征性的，而音频和视频形式将被表示为信号。
- **翻译**：第二个挑战是如何将数据从一种模式转换(映射)到另一种模式。不仅异构数据，而且模式之间的关系往往是开放的或主观的。例如，有许多正确的方法来描述一个图像，一个完美的映射可能不存在。
- **对齐**：第三个挑战是确定来自两种或两种以上不同模式的(子)元素之间的直接关系。例如，我们可能希望将菜谱中的步骤与显示正在制作的菜肴的视频对齐。为了解决这一挑战，我们需要度量不同模式之间的相似性，并处理可能的长期依赖性和模糊性。
- **融合**：第四个挑战是连接来自两个或多个模式的信息来执行预测。例如，在视听语音识别中，将唇动的视觉描述与语音信号融合，预测语音单词。来自不同模式的信息可能具有不同的预测能力和噪声拓扑结构，其中至少有一种模式可能丢失数据。
- **共同学习**：第五个挑战是在模态、它们的表示和它们的预测模型之间传递知识。这一点可以用协同训练、概念基础和零样本学习的算法来举例说明。协同学习探索了从一个模态中学习知识如何帮助在不同模态中训练的计算模型。当其中一种模式的资源有限（例如，注释数据）时，这一挑战尤其重要。   

## 多模态表示
以计算模型可以使用的格式表示原始数据一直是机器学习中的一大挑战。表示多种形式存在许多困难：如何组合来自不同来源的数据；如何处理不同级别的噪声；以及如何处理丢失的数据。以有意义的方式表示数据的能力对于多模式问题至关重要，并且是任何模型的主干。    
Bengio发现一个好的表示应该具有属性：smoothness, temporal and spatial coherence, sparsity, and natural clustering amongst others.后人还发现了其他的理想属性：similarity in the representation space
should reflect the similarity of the corresponding concepts,
the representation should be easy to obtain even in the
absence of some modalities, and finally, it should be possible
to fill-in missing modalities given the observed ones.    
目前大部分的视觉描述都是通过神经网络(CNN)等神经结构从数据中学习的，自然语言使用利用单词上下文的数据驱动的单词嵌入来表示。以前的多模态表示就是简单地将这些但模态表示拼接起来。   
为了帮助理解工作的广度，我们提出了两类多模态表示:联合和协调。联合表示将单模态信号组合到同一个表示空间中，而协调表示单独处理单模态信号，但对其施加一定的相似性约束，使其达到我们所说的协调空间
<div align=center><img src="..\image\MMML\5b1a794b8437af7ee3857a9ff1aa9e7.jpg" width="300"></div> 
  
  - 联合表示   
  联合表示法主要（但不是唯一）用于在训练和推理步骤中同时存在多模态数据的任务。联合表示的最简单示例是单个模态特征的串联。在本节中，我们讨论了创建联合表示的更先进的方法，首先是神经网络，然后是图形模型和循环神经网络。   
  神经网络已成为一种非常流行的单模态数据表示方法。它们用于表示视觉、声学和文本数据，并且越来越多地用于多模态领域。由于深层神经网络的多层性，假设每一层后续的神经网络以更抽象的方式来表示数据，因此通常使用最后一层或倒数第二层神经网络作为一种数据表示形式。为了使用神经网络构建一个多模态表示，每个模态都从几个单独的神经层开始，然后是一个隐藏层，该层将模态投射到一个共同空间。由于神经网络需要大量带标签的训练数据，因此通常使用自动编码器对无监督数据进行预训练。   
  概率图形模型是另一种通过使用潜在随机变量来构造表示的常用方法。基于图形模型的表示最流行的方法是受限玻尔兹曼机：deep Boltzmann machines (DBM)，将restricted Boltzmann machines (RBM)堆叠起来作为构建块。与神经网络类似，DBM的每个连续层都期望在更高的抽象级别上表示数据。DBMs的吸引力来自于他们不需要监督数据进行训练的事实。  
  到目前为止，RNNs主要用于表示单模态的单词、音频或图像序列，在语言领域取得了很大的成功。   
  - 协同表示   
  联合多模表示的一种替代方法是协同表示。我们不是将模态一起投影到一个联合空间中，而是为每个模态学习单独的表示，但是通过一个约束来协调它们。我们从强调表示之间的相似性的协调表示开始讨论，接着讨论在结果空间上加强结构的协调表示    
  在本节中，我们确定了两种主要的多模态表示形式——联合和协调。联合表示将多模态数据投射到一个公共空间中，最适合在推理过程中出现所有模态的情况。它们被广泛用于AVSR、情感和多模手势识别。另一方面，协调表示法将每个模态投影到一个单独但协调的空间中，使其适用于测试时只有一个模态的应用，例如：多模态检索和翻译（第4节）、接地（第7.2节）和零镜头学习（第7.2节）。最后，虽然联合表示用于构建两种以上模态的表示，但到目前为止，协调空间主要限于两种模态。   
  <div align=center><img src="..\image\MMML\e2ef507aad54ca7c344e7efc8e6b4cc.jpg" width="300"></div> 
