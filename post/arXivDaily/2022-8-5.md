Constructing Balance from Imbalance for Long-tailed Image Recognition   
长尾图像识别对深度学习系统提出了巨大的挑战，因为大多数图像之间的不平衡（负责人）班级和少数民族针对数据驱动的深度神经网络（Deep Neural Networks，DNN）中数据不平衡问题，提出了一种基于数据分布、特征空间、模型设计等多个角度的数据不平衡模型学习方法，我们建议在分类器学习之前，从之前忽略的平衡标签空间的角度来面对头尾偏差的瓶颈，为了减轻头尾偏差，我们提出了一种通过逐步调整标签空间和划分头尾类的简洁范例，通过灵活的数据过滤和标签空间映射，我们可以很容易地将我们的方法嵌入到大多数分类模型中，特别是解耦的训练方法中.此外，我们发现不同的特征在不同的归纳偏差下头尾类的可分性不同，实验结果表明，该方法能有效地提高状态—状态分类器的性能，并为长尾特征学习提供了一种有效的方法. arts上的不同类型的性能指标评测。代码可从https： github.com silicx DLSA.     
解决一些特定的数据（长尾数据）的训练问题

-----
Open-world Contrastive Learning   
近年来，对比学习的研究取得了令人瞩目的成就.然而，绝大多数的方法都局限于封闭世界的环境.本文通过将表征学习引入开放世界的环境，丰富了表征学习的研究领域.在开放世界环境中，来自新类别的未标记样本可以自然地出现.为了弥补这一差距，我们引入了一个新的学习框架，开放世界对比学习（OpenCon）。OpenCon解决了学习已知类和新类的紧凑表示的挑战，并促进了新发现的过程。我们在具有挑战性的基准数据集上演示了OpenCon的有效性，并建立了有竞争力的性能。在ImageNet数据集上，OpenCon在新分类精度和总体分类精度上分别比目前最好的方法高出11.9%和7.4%，我们希望我们的工作将为今后解决这一重要问题打开新的大门。      
从封闭环境到开放环境

-----
Towards Understanding Mixture of Experts in Deep Learning    
专家组合（MoE）层是一种由路由器控制的稀疏激活模型，在深度学习领域取得了巨大的成功，但对这种体系结构的理解仍然是一个谜.该文提出了一种基于MoE层的深度学习模型，我们正式地研究了MoE层如何改善神经网络的学习性能以及为什么混合模型不会崩溃成单一模型。我们的经验结果表明，潜在问题的聚类结构和专家的非线性是MoE成功的关键，为了进一步理解这一点，我们考虑了一个具有内在聚类结构的挑战性分类问题，而在MoE层，通过选择专家作为两层非线性卷积神经网络，（CNNs），证明了该问题是可以成功学习的，进一步证明了路由器可以学习簇中心的特征，将输入的复杂问题分解为单个专家可以解决的简单的线性分类子问题，这是正式理解用于深度学习的MoE层的机制的第一个结果。    
MoE layer的使用   

-----
