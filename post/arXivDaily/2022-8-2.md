Neural Knowledge Bank for Pretrained Transformers    
预训练的Transformers对事实知识的记忆能力对于知识密集型的下游任务（如闭卷问答）至关重要.已有的研究表明，预训练的Transformers能够在一定程度上回忆或利用预训练语料库中出现的事实知识.然而，由于模型容量的限制，预先训练的模型记忆事实知识的能力也是有限的。（2022）发现前馈网络Transformers的FFN以类似记忆的方式储存事实知识，我们提出了一个神经知识库（NKB）来存储预先训练好的Transformer的额外事实知识，具体地说，我们也把FFN看作是键值记忆，并在知识注入过程中对原模型进行固定，将实际知识注入到扩展的记忆槽中，FFN作为键值记忆的观点使得NKB具有很强的可解释性.我们使用了三个闭卷问答数据集来展示我们强大的存储额外事实知识的能力.同时，我们通过两个典型的自动生成任务——自动摘要和机器翻译证明了NKB不会降低预训练模型的通用语言生成能力，我们深入分析了NKB以揭示其工作机制并以一种人类可读的方式给出了其键值的含义，我们执行直接更新NKB中的事实知识而不需要任何附加训练的初步尝试。    
使用一个新的网络讲预训练的知识存储起来   

-----    
DictBERT: Dictionary Description Knowledge Enhanced Language Model Pre-training via Contrastive Learning   
尽管预先训练的语言模型（PLM）在各种自然语言处理方面取得了一流的性能（NLP）任务，在处理知识驱动的任务时，他们表现出缺乏知识。尽管为将知识注入PLM做出了许多努力，但这个问题仍然没有解决。为了解决这个挑战，我们提出 textbf{DictBERT}，在预训练阶段，我们提出了两个新的预训练任务，通过对比学习将词典知识注入到PLM中：在微调中，我们使用预先训练的DictBERT作为插件知识库（KB）来检索输入序列中所标识的条目的隐式知识，并将检索到的知识注入到输入中以通过新颖的额外跳跃注意机制来增强其表示。在一系列知识驱动的语言理解任务（包括NER、关系抽取、CommonsenseQA、OpenBookQA和GLUE）上对我们的方法进行了评估，实验结果表明，我们的方法能够显著提高典型PLM：该算法对BERT-large的性能分别提高了0.5%、2.9%、9.0%、7.1%和3.3%，对RoBERTa-large也有效    
将知识注入PTM