# 西瓜书复习

## 一 、绪论

机器学习就是基于经验做出的预判

## 二、模型评估与选择

过拟合的解决方法

- 增加数据量（多加点数据、数据增强）
- 正则化（L1、L2 Regularization, Dropout）
- 简化模型（网络结构、训练时间、限制权值、增加噪声）
- 模型融合（Bagging、Boosting、Dropout）
- 贝叶斯方法

欠拟合的解决方法

- 数据未做归一化处理
- 没有使用任何正则化方法
- 使用了一个太大的batch size
- 使用了一个错误的学习率
- 在最后一层使用错误的激活函数
- 使用了一个太深的神经网络
- 隐藏层神经元的数量设置不正确

训练测试集划分方法

- 留出法
- K折交叉验证
- 自助法

模型的评价指标

[所有的机器学习模型的评价指标都有哪些？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/315563876)

超参数调优
- 网格搜索
- 随机搜索
- 贝叶斯优化算法：会利用先验知识来更新目标函数形状

## 线性模型

[数据降维算法LDA & PCA](https://zhuanlan.zhihu.com/p/166865253)

LDA选择分类性能最好的投影方向（最大化类间距离，最小化类内距离），而PCA选择样本点投影具有最大方差的方向（因为信噪比越大，数据质量越好）

无监督任务使用PCA降维，有监督任务使用LDA降维。

## 决策树

划分选择

- 信息增益
- 增益率
- 基尼指数

常用决策树算法
- ID3：最大信息增益
- C4.5：最大信息增益比
- CART：最大基尼指数


决策树的剪枝策略最基本的有两种：预剪枝（pre-pruning）和后剪枝（post-pruning）

[决策树原理](https://blog.csdn.net/GreenYang5277/article/details/104500739)

## 神经网络

模拟生物神经系统对真实世界物体所作出的交互反应

BP算法（基于梯度下降策略）

BP网络过拟合的解决方法

- 早停
- 正则化（在误差目标函数中增加一个用于描述网络复杂度的部分）

跳出局部极小

- 多组初始化
- 模拟退火
- 随机梯度下降
- 遗传算法

级联相关网络（将网络结构也当作学习的目标之一，优点是加速训练，无需确定网络结构，这个思想可以用于论文）

凸优化问题：所有的局部极小值都是全局最小值

梯度下降算法
- GD
- SGD
- minibatchSGD
- Momentum SGD
- AdaGrad
- Adam

激活函数
- Sigmoid
- Tanh
- ReLU

归一化
- Batch Normalization
- Layer Normalization

## 支持向量机

它通过在样本空间中找到一个划分超平面，将不同类别的样本分开，同时使得两个点集到此平面的最小距离最大，两个点集中的边缘点到此平面的距离最大（用核函数进行计算）

- 硬间隔
- 软间隔
- 非线性

## 贝叶斯分类器

最小化分类错误率

极大似然估计（根据数据采用来估计概率分布参数）

朴素贝叶斯分类器（属性条件独立性假设）

半朴素贝叶斯分类器（考虑一部分属性依赖）

贝叶斯网

EM算法（迭代式方法、坐标下降法、非梯度下降法）

## 集成学习

Boosting（序列化方法）(迭代法)

将多个弱分类器组装成一个强分类器。每轮迭代生成的基模型，主要提升前一代基模型表现不好的地方。（不断迭代弱分类器，从而降低 Bias。因此，适用于 Low Variance & High Bias 的模型）

- AdaBoost: 通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。
- XGBoost: AdaBoost用错分数据点来识别问题，通过调整错分数据点的权重来改进模型。Gradient Boosting通过负梯度来识别问题，通过计算负梯度来改进模型。（是对GBDT的工程优化）

Adaboost 与 GBDT 两者 boosting 的不同策略是两者的本质区别。

Adaboost强调Adaptive（自适应），通过不断修改样本权重（增大分错样本权重，降低分对样本权重），不断加入弱分类器进行boosting。

GBDT 则是在确定损失函数后，本轮 cart 树的拟合目标就是沿着损失函数相对于前一轮组合树模型的负梯度方向进行拟合，也就是希望最快速度地最小化预测值与真实值之间的差异；当损失函数选择为 square loss 时候，其沿着负梯度方向拟合表现为拟合残差（选择其他损失函数不一定表现出拟合残差的性质）


Bagging（并行化方法）

每次使用一份训练集训练一个模型，k 个训练集共得到 k 个基模型。利用这k个基模型对测试集进行预测，将k个预测结果进行聚合（分类用投票法，回归用均值法）（主要降低 Variance，对 Bias 无明显作用。因此，适用于 High Variance & Low Bias 的模型。）


结合策略

- 平均法
- 投票法
- 学习法

多样性（误差-分歧分解，即应好而不同）

多样性增强（引入随机性，即对数据样本、输入属性、输出表示、算法参数进行扰动）

1）Bagging + 决策树 = 随机森林

2）AdaBoost + 决策树 = 提升树

3）Gradient Boosting + 决策树 = GBDT

## 聚类

自动形成簇结构、但是对应的语义需要自己定义

性能度量（簇内相似度高、簇间相似度低）

距离计算（闵可夫斯基计算）

原型聚类（极为常用）

- K-means（使得簇内样本尽量紧密）（k-means++ 选择迭代距离远的为中心点）
- 学习向量量化（假设样本具有类别标记）
- 高斯混合聚类（使用概率模型）

密度聚类（假设聚类结构能通过样本分布的紧密程度决定）

层次聚类

## 降维与度量学习

kNN（懒惰学习）

低维嵌入（数据是高维的，但是学习任务是低维的）

主成分分析法

核化线性降维

流形学习

度量学习（对距离度量进行学习）

## 特征选择与稀疏学习

特征选择（减轻维数灾难、降低学习难度，但是要确保不丢失重要特征）

- 过滤式
- 包裹式
- 嵌入式

稀疏表示（矩阵）与字典表示（为稠密的样本找到稀疏的字典）

压缩感知（用部分信息恢复全部信息）

## 计算学习理论

目的是分析学习任务的困难本质，为学习算法提供理论保证，并根据分析结果指导算法设计

## 半监督学习

生成式方法（未标记数据的标记视为缺失参数）

半监督SVM（伪标记的产生基于相似数据具有相同标记的假设）

图半监督学习（把数据集映射为图）

基于分歧的方法

## 概率图模型
概念：用关系图来表示概率分布

分为贝叶斯网络和马尔可夫网络两大类

隐马尔科夫模型

马尔可夫随机场

条件随机场

## 规则学习

规则是语义明确的

目标是产生一个能覆盖尽可能多样例的规则集

## 强化学习

通常用马尔可夫决策过程来描述