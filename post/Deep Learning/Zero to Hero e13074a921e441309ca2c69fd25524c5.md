# Zero to Hero

本文意在对研究方向进行一个完整的调研，即对多模态预训练做一个完整的梳理。目前来说，多模态主要涉及语言和视觉。因此，我们同时对NLP和CV也需要有一个宏观的把握

## 多模态预训练

### 资源

https://github.com/phellonchen/awesome-Vision-and-Language-Pre-training

https://github.com/zhjohnchan/awesome-vision-and-language-pretraining

https://github.com/VainF/Awesome-Contrastive-Learning

https://github.com/pliang279/awesome-multimodal-ml

https://github.com/Eurus-Holmes/Awesome-Multimodal-Research

https://github.com/thuiar/AWESOME-MSA

https://github.com/EvelynFan/AWESOME-MER

https://github.com/Yutong-Zhou-cv/Awesome-Multimodality

https://github.com/ZihengZZH/awesome-multimodal-machine-translation

### 综述

参考资料

[[2202.09061] VLP: A Survey on Vision-Language Pre-training (arxiv.org)](https://arxiv.org/abs/2202.09061)

[视觉-语言预训练(Vision-Language Pretraining)综述 | 一起打怪升级呀 (nicehuster.github.io)](https://nicehuster.github.io/2022/06/05/VLP/)

[[1705.09406] Multimodal Machine Learning: A Survey and Taxonomy (arxiv.org)](https://arxiv.org/abs/1705.09406)

在计算机视觉与自然语言处理预训练模型层出不穷的今天，**多模态预训练模型**也悄然浮出水面。目前来说，主要指的是**图像-文本**和**视频-文本**预训练。

VLP主要通过对大规模数据的预训练来学习不同模态之间的语义对应关系。例如，在图文预训练中，我们希望模型将文本中的“狗”与图像中的“狗”联系起来。在视频文本预训练中，我们期望模型将文本中的对象/动作映射到视频中的对象/动作。为了实现这一目标，需要巧妙地设计VLP对象和模型体系结构，以允许模型挖掘不同模式之间的关联。

我们可以分为五个方面来理解这个领域，分别是

- 特征提取
    - 图像特征提取
        1. OD-based Region Features (OD-RFs)：使用预训练的目标检测模型识别图像中目标区域，并提取每个区域的表示，来提取视觉特征，常用的是Faster RCNN；
        2. CNN-based Grid Features (CNN-GFs)：直接用CNN对整张图提取视觉特征获得网格特征；
        3. ViT-based Patch Features (ViT-PFs)：使用ViT将图片压平成序列，对于transformer类型的encoder的输入比较友好；
    - 视频特征提取
        
        一般把视频当做M帧组成的图像信息。VLP模型利用上述图像特征提取方法提取frame特征。
        
    - 文本特征提取
        
        对于文本特征，一般使用Bert进行特征提取。VLP模型首先将输入的句子分割成一系列字词。然后再序列的开头和结尾处插入序列开始和结束标记。
        
- 模型架构
    - 从多模态融合上，可以划分为：Single-stream和Dual-stream
        
        单流架构是指将文本和视觉特征连接在一起，然后馈送到单个Transformer块中。双流架构是指文本和视觉特征不是串联在一起而是独立地发送到两个不同的Transformer块中。单流架构一般来说更加parameter-efficient。双流架构一般采用上图虚线所表示的cross attention进行特征交互。
        
    - 从整体结构设计上，可以划分为：Encoder-Only和Encoder-Decoder
        
        许多VLP模型采用encoder-only的体系结构，其中跨模态表示被直接馈送到输出层以生成最终输出。相比之下，其他VLP模型主张使用encoder-decoder体系结构，其中跨模态表示首先被馈送到decoder，然后被馈送到输出层。
        
    
- 预训练目标
    - ****Completion****
        - **Masked Language Modeling**
        - **Prefix Language Modeling**
        - **Masked Vision Modeling(Masked Features Regression & Masked Feature Classification)**
    - ****Matching****
        - **Vision-Language Matching**
        - **Vision-Language Contrastive Learning**
        - **Word-Region Alignment(可用于获取细粒度信息)**
    - ****Temporal(通过对中断的输入序列重新排序来学习良好的representation)****
        - **Frame Order Modeling**
    - ****Particular(为了更好地适应下游任务，VLP可以选择一些针对该任务的预训练目标)****
        - V**isual Question Answering**
        - V**isual Captioning**
- 预训练数据集
    
    ![Untitled](Zero%20to%20Hero%20e13074a921e441309ca2c69fd25524c5/Untitled.png)
    
- 下游任务
    
    ![Untitled](Zero%20to%20Hero%20e13074a921e441309ca2c69fd25524c5/Untitled%201.png)
    
- SOTA
    
    ![Untitled](Zero%20to%20Hero%20e13074a921e441309ca2c69fd25524c5/Untitled%202.png)
    
- Future Works
    - Incorporating Acoustic Information
        
        声音虽然和文字有点重复了，但是它可以带来一些补充信息（如extra emotion information, acoustic boundary information）加入声音模态，也可以使得模型可以处理这一模态的输入输出，使得模型更加地“智能”
        
    - Knowledgeable and Cognitive Learning
        
        目前的VLP本质上还是去fit大规模的预训练数据，可以考虑引入external common sense world knowledge and illustrative situational knowledge，对模型进行增强
        
    - Model Compression and Acceleration
        
        VLP模型往往过于庞大，因此在实际应用中十分受限。可以使用parameter sharing, model pruning, knowledge distillation and model quantization等方法对模型进行压缩和加速
        
    - Out-of-domain Pretraining
        
        将VLP模型学习到的知识和表示更好地用于那些downstream tasks with unknown data distributions
        
    - Advanced Model Architecture
        
        现在VLP通常会用transformer-based architectures，但是是否就是最佳选择了呢？例如最近的diffusion model在文本生成等任务上展现了不错的效果，我们是否可以考虑将其应用到VLP上面？
        

常见的多模态研究可分为以下五类

1. **表征**。如何挖掘模态间的互补性或独立性以表征多模态数据。
2. **翻译**。学习一个模态到其他模态的映射。例如：image captioning。
3. **对齐**。将多模态数据的子元素进行对齐。例如phrase grounding任务：将一幅图中的多个物体与一段话中的短语(或单词)进行对齐。在学习表征或翻译时也可能隐式地学习对齐。
4. **融合**。融合两个模态的数据，用来进行某种预测。例如：Visual Question Answering需融合图像和问题来预测答案；Audio-visual speech recognition需融合声音和视频信息用以识别说话内容。
5. **共同学习(co-learning)**。模态间的知识迁移。使用辅助模态训练的网络可以帮助该模态的学习，尤其是该模态数据量较小的情况下。

下面给出多模态应用的种类及其亟需解决的技术难点

![Untitled](Zero%20to%20Hero%20e13074a921e441309ca2c69fd25524c5/Untitled%203.png)

一、表征

技术难点

- 如何结合来源不同的异质数据
- 如何处理不同模态的不同噪声等级
- 测试样本的某种模态缺失怎么办

解决方案

- **Joint结构：**注重捕捉多模态的互补性，网络优化目标是某种预测任务的性能(LXMERT, Oscar, UNITER)(适合在推断时所有模态都能提供的情况)
- **Coordinated结构：**建模多种模态数据间的相关性，网络的优化目标是这种协作关系(通常是相似性，即最小化cosine距离等度量)(CLIP, BriVL)(适合推断时只有部分模态能提供的情况)
    
    ![Untitled](Zero%20to%20Hero%20e13074a921e441309ca2c69fd25524c5/Untitled%204.png)
    

现有工作

![Untitled](Zero%20to%20Hero%20e13074a921e441309ca2c69fd25524c5/Untitled%205.png)

二、翻译

技术难点

- 多模态翻译方法面临的一个主要挑战是很难对其进行评价

解决方案

- Example-based(借助于词典进行翻译，词典一般指训练集中的数据对)
- Generative Approaches(在给定单模源实例的情况下，多模翻译的生成方法构造了能够执行多模翻译的模型)

![Untitled](Zero%20to%20Hero%20e13074a921e441309ca2c69fd25524c5/Untitled%206.png)

现有工作

![Untitled](Zero%20to%20Hero%20e13074a921e441309ca2c69fd25524c5/Untitled%207.png)

三、对齐

技术难点

- 仅有少量数据集包含显式的对齐标注
- 跨模态度量难以设计
- 可能存在多种对齐，也可能存在某些元素无法在其他模态中找到

解决方案

- Explicit Alignment(如果一个模型的优化目标是最大化多模态数据的子元素的对齐程度，则称为显示对齐)
- Implicit Alignment(如果模型的最终优化目标不是对齐任务，对齐过程仅仅是某个中间(或隐式)步骤，则称为隐式对齐)(最受欢迎的方式是**基于注意力机制的对齐，**我们对两种模态的子元素间求取注意力权重矩阵**，**可视为隐式地衡量跨模态子元素间的关联程度。在图像描述，这种注意力被用来判断生成某个单词时需要关注图像中的哪些区域。在视觉问答中，注意力权重被用来定位问题所指的图像区域。很多基于深度学习的跨模态任务都可以找到跨模态注意力的影子)

现有工作

![Untitled](Zero%20to%20Hero%20e13074a921e441309ca2c69fd25524c5/Untitled%208.png)

四、融合

技术难点

- 优点：第一，能够访问观察同一现象的多种模式，可能会使预测更加可靠。第二，能够访问多种模式可能允许我们捕获互补的信息。第三，其中一种模态缺失时，多模态系统仍然可以运行
- 同模态的序列信息可能没有对齐
- 信号间的关联可能只是补充(仅提高鲁棒性而无法增大信息量)而不是互补
- 不同数据可能存在不同程度的噪声

解决方案

- Model-agnostic approaches
- Model-based approaches

现有工作

![Untitled](Zero%20to%20Hero%20e13074a921e441309ca2c69fd25524c5/Untitled%209.png)

五、共同学习

技术难点

- 通过从另一个(资源丰富的)模态中获取知识来帮助(资源贫乏的)模态建模。当其中一种模式的资源有限时(缺少带注释的数据、有噪声的输入和不可靠的标签)，它尤其重要

解决方案

- Parallel data
- Non-parallel data
- Hybrid data

![Untitled](Zero%20to%20Hero%20e13074a921e441309ca2c69fd25524c5/Untitled%2010.png)

现有工作

![Untitled](Zero%20to%20Hero%20e13074a921e441309ca2c69fd25524c5/Untitled%2011.png)

## 自然语言处理

### 资源

https://github.com/crownpku/Awesome-Chinese-NLP

https://github.com/keon/awesome-nlp

https://github.com/lonePatient/awesome-pretrained-chinese-nlp-models

https://github.com/cedrickchee/awesome-transformer-nlp

https://github.com/Jiakui/awesome-bert

https://github.com/haiker2011/awesome-nlp-sentiment-analysis

### 综述

参考资料

[Modern Deep Learning Techniques Applied to Natural Language Processing by Authors (nlpoverview.com)](https://nlpoverview.com/)

[Tracking Progress in Natural Language Processing | NLP-progress (nlpprogress.com)](https://nlpprogress.com/)

[An introduction to Deep Learning in Natural Language Processing: Models, techniques, and tools - ScienceDirect](https://www.sciencedirect.com/science/article/pii/S0925231221010997)

一、任务与应用

**序列分类**：输入为一组序列，每个序列包括系列tokens，对应一个类别； **相关示例**（情感分析，根据其极性对简短的文本进行分类；文本分类，找文本的主题（体育，财务…）；回答句子选择，从给定的段落/文本中选择最佳句子以回答输入问题）

**成对序列分类**：根据它们的相似性，语义和含义，比较和分类两个不同的序列，通常是一个二进制分类任务。输入为两个不同的序列，若表达相同的含义返回+1，否则-1；需要充分理解序列并提取有意义的语义表示，克服同义、多义等问题； **相关示例**（Quora问题对挑战，从Quora找到重复的问题）

**单词标注**：每个token附加一个标签，输出空间由输入的每个元素的标签序列组成； **相关示例**（命名实体识别（NER），从输入序列中确定相关实体（如名称，位置）；经典问题回答，使用输入段落涉及的概率分布，选择包含答案的span；语音部分（pos）标记，将文本中的单词标记为对应语音的特定部分（如动词，名词，形容词））

**seq2seq**：使用输入序列生成输出序列。与单词标记不同，输入序列和输出序列不直接对齐，且需要生成新句子（尽管输入输出都包含序列，但可能是不相交的集合）

二、历史与发展

总的来说，NLP所做的事情就是将原始文本编码为具有语义信息的表示，使其可以应对不同的下游任务。方法一般包括两种，即Word and sentence vectors和Pre-trained Transformer models

1、过去十年中的主要问题之一是定义token，句子和文件的适当有效表示

- One-hot
- Word2Vec(提出的改进包括GloVe、fastText）
- 有意义的文档和句子级别表示的新方法，可以分为两个类别，即无监督文档嵌入技术（通常受到Word2vec的启发）和监督方法
    
    **无监督方法**：通过单词矢量的简单平均池化得到句子向量；直接扩展Word2Vec的不同方法（如Doc2Vec /ParagraphVector）；基于skip-gram相同结构的skip-thought向量（将原子单元由单词变为句子，试图根据目标句子重建上下文）；基于带GRU单元RNN的编码解码结构；fastSent（扩展skip-thought向量，使用CNN编码器和RNN解码器的组合）
    
    **监督方法**：大多基于递归或卷积神经网络，通常在词向量之上构建一个神经网络，结合预训练词嵌入的特性、神经架构的弹性和监督的强度；用于序列分类的 CNN 的典型结构：将静态词向量作为输入， 然后由卷积层学习单词之间的语义关系；相关应用有神经机器翻译（MT），将seq2seq作为编码-解码器，将句子而不是短语作为输入，但存在长句子信息不能被压缩在固定长度向量中的问题，因此添加注意力机制。
    
- 动态嵌入：ELMo、BERT、GPT

2、具有Transformer架构的预训练语言模型

这类模型表现好的原因有两点

- the architecture strongly based on self-attention mechanisms that allow to
read and to keep track of the whole input sequence
- the pre-training that allows the network to read and to (at least apparently) understand a text, its semantic and the meaning.

受BERT启发，提出的其他预训练Transformer，如RoBERTa、ALBERT和DistilBERT（架构相同，细微差别）；基于相同概念的其他方法有GPT、Transformer-XL及其扩展XLNet

三、挑战与未来

- 一般的Transformer架构因为计算复杂度是指数增长的，因此往往序列长度限制在512token(解决方案:Transformer-XL, Longformer)
- 探寻高效的模型与训练方法，包括lighter Transformers and distillation approaches
- 未来的方向包括(i)cyber-security, such as fake news detection, (ii) industrial applications, such as virtual assistants (Alexa, Siri...), and (iii) text generation based for instance on recent variational autoencoders or Generative Adversarial Networks.

## 计算机视觉

### 资源

https://github.com/dk-liang/Awesome-Visual-Transformer

https://github.com/ahong007007/awesomeCV

https://github.com/lcylmhlcy/Awesome-algorithm-interview

https://github.com/xavier-zy/Awesome-pytorch-list-CNVersion

https://github.com/52CV/CV-Surveys

https://github.com/mli/paper-reading

### 综述

参考资料

[[2206.06488] Multimodal Learning with Transformers: A Survey (arxiv.org)](https://arxiv.org/abs/2206.06488)

[(106条消息) [转]计算机视觉入门系列（一） 综述_Shawn Chou的博客-CSDN博客](https://blog.csdn.net/qq_42899794/article/details/98891026)

[计算机视觉发展史 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/142927311)

[一文带你了解卷积神经网络CNN的发展史 | 机器之心 (jiqizhixin.com)](https://www.jiqizhixin.com/articles/2019-05-27-4)

[(106条消息) 计算机视觉知识点整理（上） 基础篇（持续更新）_计算机视觉基础知识_吸欧大王的博客-CSDN博客](https://blog.csdn.net/weixin_36714575/article/details/115698613)

[(106条消息) 计算机视觉知识点整理（中） 模型篇（持续更新）_计算机视觉模型_吸欧大王的博客-CSDN博客](https://blog.csdn.net/weixin_36714575/article/details/114284660)

[(106条消息) 计算机视觉知识点整理（下）轻量化篇（持续更新）_吸欧大王的博客-CSDN博客](https://blog.csdn.net/weixin_36714575/article/details/115698788)

[近十年计算机视觉领域经典模型原文及总结（CNN-Backbone篇） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/574869552)

一、简介

传统的计算机视觉对待问题的解决方案基本上都是遵循： 图像预处理 → 提取特征 → 建立模型（分类器/回归器） → 输出 的流程。 而在深度学习中，大多问题都会采用端到端（End to End）的解决思路，即从输入到输出一气呵成。

![Untitled](Zero%20to%20Hero%20e13074a921e441309ca2c69fd25524c5/Untitled%2012.png)

二、发展

**1、20世纪50年代，主题是二维图像的分析和识别**

1959年，Russell和他的同学研制了一台可以把图片转化为被二进制机器所理解的灰度值的仪器——这是第一台数字图像扫描仪，处理数字图像开始成为可能。

**2、20世纪60年代，开创了三维视觉理解为目的的研究**

从二维图片中推导三维信息

**3、20世纪70年代，出现课程和明确理论体系**

**4、20世纪80年代 ，独立学科形成，理论从实验室走向应用**

1980年，日本计算机科学家Kunihiko Fukushima在Hubel和Wiesel的研究启发下，建立了一个自组织的简单和复杂细胞的人工网络——Neocognitron，包括几个卷积层（通常是矩形的），他的感受野具有权重向量（称为滤波器）。这些滤波器的功能是在输入值的二维数组（例如图像像素）上滑动，并在执行某些计算后，产生激活事件（2维数组），这些事件将用作网络后续层的输入。Fukushima的Neocognitron可以说是**第一个神经网络**，是现代 CNN 网络中卷积层+池化层的最初范例及灵感来源。

**5、20世纪90年代，特征对象识别开始成为重点**

1999年， David Lowe 发表《基于局部尺度不变特征（SIFT特征）的物体识别》，标志着研究人员开始停止通过创建三维模型重建对象，而转向**基于特征的对象识别**。

**6、21世纪初，图像特征工程,出现真正拥有标注的高质量数据集**

2005年，由Dalal & Triggs提出来方向梯度直方图，**HOG**（Histogramof Oriented Gradients）应用到行人检测上。是目前计算机视觉、模式识别领域很常用的一种描述图像局部纹理的特征方法。

2006年左右，Geoffrey Hilton和他的学生发明了用GPU来优化深度神经网络的工程方法，并发表在《Science》和相关期刊上发表了论文，首次提出了“深度信念网络”的概念。他给多层神经网络相关的学习方法赋予了一个新名词–“**深度学习**”。随后深度学习的研究大放异彩，广泛应用在了图像处理和语音识别领域【3】，他的学生后来赢得了2012年ImageNet大赛，并使**CNN**家喻户晓。

2009年，由Felzenszwalb教授在提出基于HOG的deformable parts model(DPM)，可变形零件模型开发，它是深度学习之前最好的**最成功的objectdetection & recognition算法**。它最成功的应用就是检测行人，目前DPM已成为众多分类、分割、姿态估计等算法的核心部分，

**7、2010年-至今 深度学习在视觉中的流行，在应用上百花齐放**

2009年，李飞飞教授等在CVPR2009上发表了一篇名为《ImageNet: A Large-Scale Hierarchical Image Database》的论文，发布了**ImageNet**数据集，这是为了检测计算机视觉能否识别自然万物，回归机器学习，克服过拟合问题，经过三年多在筹划组建完成的一个大的数据集。

2012 年，Alex Krizhevsky、Ilya Sutskever 和 Geoffrey Hinton 创造了一个“大型的深度卷积神经网络”，也即现在众所周知的 **AlexNet**，赢得了当年的 ILSVRC。机器识别的错误率从25%左右。降低了百分之16%左右，跟人类相比差别不大。是自那时起，CNN 才成了家喻户晓的名字。

2014年，蒙特利尔大学提出生成对抗网络（**GAN**）：拥有两个相互竞争的神经网络可以使机器学习得更快。一个网络尝试模仿真实数据生成假的数据，而另一个网络则试图将假数据区分出来。随着时间的推移，两个网络都会得到训练，生成对抗网络（GAN）被认为是计算机视觉领域的重大突破。

2017-2018 年**深度学习框架**的开发发展到了成熟期。PyTorch 和 TensorFlow 已成为首选框架，它们都提供了针对多项任务（包括图像分类）的大量预训练模型。

三、经典模型

参考李沐的经典论文集

https://github.com/mli/paper-reading

## 深度学习

### 资源